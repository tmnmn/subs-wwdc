WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:19:59.726 --> 00:20:01.176 A:middle
那运动传感器

00:20:01.176 --> 00:20:02.706 A:middle
会检测到

00:20:02.706 --> 00:20:04.476 A:middle
向上或向下的运动趋势

00:20:04.476 --> 00:20:06.766 A:middle
但视觉上 它所处的环境

00:20:06.816 --> 00:20:07.566 A:middle
并没有变化

00:20:08.066 --> 00:20:11.846 A:middle
那你要如何获得

00:20:11.846 --> 00:20:13.966 A:middle
用户在使用

00:20:14.036 --> 00:20:16.926 A:middle
你的 App 时的体验的反馈呢

00:20:18.236 --> 00:20:20.586 A:middle
ARKit 会监控它自己的跟踪表现

00:20:21.186 --> 00:20:22.776 A:middle
我们在 ARKit 中应用了机器学习

00:20:23.076 --> 00:20:24.536 A:middle
ARKit 在成千上万组

00:20:24.536 --> 00:20:26.546 A:middle
的数据学习中获得提升

00:20:26.546 --> 00:20:28.706 A:middle
这些数据中包括在不同

00:20:28.706 --> 00:20:30.266 A:middle
情况下跟踪的表现

00:20:31.776 --> 00:20:33.276 A:middle
为了训练出一个能够

00:20:33.276 --> 00:20:35.036 A:middle
告诉你跟踪表现的分类器

00:20:35.036 --> 00:20:36.846 A:middle
我们使用了一些注释

00:20:36.846 --> 00:20:39.486 A:middle
比如视觉内容的数量

00:20:39.566 --> 00:20:41.136 A:middle
在图像中跟踪到的可视特征

00:20:41.936 --> 00:20:44.346 A:middle
以及设备当时的运动速率

00:20:45.496 --> 00:20:47.866 A:middle
在运作时 跟踪的表现

00:20:47.866 --> 00:20:50.636 A:middle
是由这些参数

00:20:50.636 --> 00:20:51.746 A:middle
所决定的

00:20:52.616 --> 00:20:55.116 A:middle
在这段视频中

00:20:55.116 --> 00:20:57.026 A:middle
我们将镜头遮住

00:20:57.026 --> 00:20:58.406 A:middle
但保持活动和

00:20:58.406 --> 00:21:00.846 A:middle
对环境的探索

