WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:00:59.756 --> 00:01:03.826 A:middle
content of images, or perhaps,

00:01:04.726 --> 00:01:05.876 A:middle
analyze some text.

00:01:08.696 --> 00:01:11.266 A:middle
What could you do if your app

00:01:11.266 --> 00:01:13.026 A:middle
could reason about audio or

00:01:13.026 --> 00:01:16.536 A:middle
music, or interpret your users'

00:01:16.536 --> 00:01:17.956 A:middle
actions based on their motion

00:01:17.956 --> 00:01:21.506 A:middle
activity, or even transform or

00:01:21.506 --> 00:01:22.976 A:middle
generate new content for them?

00:01:24.416 --> 00:01:26.106 A:middle
All of this, and much, much

00:01:26.106 --> 00:01:28.336 A:middle
more, is easily within reach.

00:01:29.046 --> 00:01:30.356 A:middle
And that's because this type of

00:01:30.356 --> 00:01:32.496 A:middle
functionality can be encoded in

00:01:32.496 --> 00:01:33.356 A:middle
a Core ML model.

00:01:35.146 --> 00:01:36.786 A:middle
Now if we take a peek inside one

00:01:36.786 --> 00:01:39.126 A:middle
of these, we may find a neural

00:01:39.126 --> 00:01:41.396 A:middle
network, tree ensemble, or some

00:01:41.396 --> 00:01:42.426 A:middle
other model architecture.

00:01:43.686 --> 00:01:44.876 A:middle
They may have millions of

00:01:44.926 --> 00:01:46.876 A:middle
parameters, the values of which

00:01:46.876 --> 00:01:47.976 A:middle
have been learned from large

00:01:47.976 --> 00:01:48.666 A:middle
amounts of data.

00:01:50.596 --> 00:01:52.686 A:middle
But for you, you could focus on

00:01:52.686 --> 00:01:53.446 A:middle
a single file.

00:01:54.166 --> 00:01:55.716 A:middle
You can focus on the

00:01:55.716 --> 00:01:57.566 A:middle
functionality it provides and

00:01:57.566 --> 00:01:59.816 A:middle
the experience it enables rather

00:01:59.816 --> 00:02:00.816 A:middle
than those implementation

