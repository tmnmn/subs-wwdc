WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:18:58.206 --> 00:19:01.206 A:middle
possible to have the same model

00:19:01.426 --> 00:19:03.116 A:middle
to support two different

00:19:03.116 --> 00:19:03.676 A:middle
functions.

00:19:04.406 --> 00:19:06.006 A:middle
For example, you can train a

00:19:06.006 --> 00:19:06.906 A:middle
multi-task model.

00:19:06.906 --> 00:19:09.316 A:middle
And multi-task models has been

00:19:09.316 --> 00:19:10.936 A:middle
trained to perform multiple

00:19:10.936 --> 00:19:11.626 A:middle
things at once.

00:19:12.446 --> 00:19:13.656 A:middle
There is an example about style

00:19:13.656 --> 00:19:14.776 A:middle
transferring, the Turi Create

00:19:14.836 --> 00:19:16.276 A:middle
session about multi-task models.

00:19:16.936 --> 00:19:19.366 A:middle
Or in some cases, you can use a

00:19:19.366 --> 00:19:20.856 A:middle
yet new feature in Core ML

00:19:21.146 --> 00:19:22.596 A:middle
called Flexible Shapes and

00:19:22.596 --> 00:19:23.076 A:middle
Sizes.

00:19:24.386 --> 00:19:27.006 A:middle
Let's go back to our Style

00:19:27.006 --> 00:19:27.726 A:middle
Transfer demo.

00:19:28.166 --> 00:19:30.516 A:middle
In Xcode we saw that the size of

00:19:30.516 --> 00:19:31.916 A:middle
the input image and the output

00:19:31.916 --> 00:19:34.826 A:middle
image was encoded in part of the

00:19:34.826 --> 00:19:35.966 A:middle
definition of the model.

00:19:36.816 --> 00:19:38.066 A:middle
But what if we want to run the

00:19:38.066 --> 00:19:39.666 A:middle
same style on different image

00:19:39.666 --> 00:19:40.346 A:middle
resolution?

00:19:41.056 --> 00:19:42.576 A:middle
What if we want to run the same

00:19:42.576 --> 00:19:44.746 A:middle
network on different image

00:19:44.746 --> 00:19:45.226 A:middle
sizes?

00:19:46.796 --> 00:19:49.156 A:middle
For example, the user might want

00:19:49.156 --> 00:19:50.726 A:middle
to see a high-definition style

00:19:50.726 --> 00:19:51.186 A:middle
transfer.

00:19:51.826 --> 00:19:53.906 A:middle
So they use -- they give us a

00:19:53.906 --> 00:19:54.906 A:middle
high-definition image.

00:19:55.646 --> 00:19:57.316 A:middle
Now if I were Core ML model all

00:19:57.316 --> 00:19:58.546 A:middle
it takes is a lower resolution

00:19:58.546 --> 00:20:00.766 A:middle
as an input, all we can do as

