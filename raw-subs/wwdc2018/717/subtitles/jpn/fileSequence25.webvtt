WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:24:55.929 --> 00:25:00.200 align:start position:29% line:-2
バックグラウンドキューで
動かすようにします

00:25:00.300 --> 00:25:04.638 align:start position:30% line:-2
そうすれば
同時にカメラを使えます

00:25:04.972 --> 00:25:10.077 align:start position:29% line:-2
撮影時 キューにタスクが
たまることは望まないので

00:25:10.177 --> 00:25:14.348 align:start position:32% line:-2
今回はバッファを
１つだけにしましたね

00:25:14.448 --> 00:25:17.084 align:start position:29% line:-1
すると うまくいきました

00:25:17.184 --> 00:25:21.955 align:start position:30% line:-2
必要なバッファだけが
動いていることを確認し

00:25:22.122 --> 00:25:28.195 align:start position:29% line:-2
終了したらリセットして
新たなバッファを使います

00:25:31.098 --> 00:25:36.970 align:start position:23% line:-2
なぜ Core MLモデルに
Visionを使うのでしょうか

00:25:38.105 --> 00:25:42.042 align:start position:25% line:-1
それには重要な理由があります

00:25:42.209 --> 00:25:45.445 align:start position:25% line:-1
モデルを思い出してみましょう

00:25:45.546 --> 00:25:49.416 align:start position:25% line:-2
299×299ピクセルという
妙なサイズの画像で―

00:25:49.516 --> 00:25:53.487 align:start position:32% line:-2
このモデルは
トレーニングをします

00:25:53.787 --> 00:25:58.959 align:start position:23% line:-2
でも 撮影される画像のサイズは
さまざまですね

00:25:59.760 --> 00:26:05.132 align:start position:30% line:-2
そこで Visionが
その画像を処理して

