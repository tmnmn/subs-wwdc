WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:25:59.793 --> 00:26:03.964 align:middle line:-2
而Vision将完成所有这些工作
它获取这些来自相机的YUV缓冲区

00:26:04.031 --> 00:26:07.234 align:middle line:-1
将其转换为RGB并缩放图像

00:26:07.301 --> 00:26:09.036 align:middle line:0
而你不必为此编写任何代码

00:26:09.436 --> 00:26:12.773 align:middle line:0
这使得用Vision驱动这些
Core ML模型

00:26:12.840 --> 00:26:14.541 align:middle line:0
来处理图像请求变得更加容易

00:26:17.411 --> 00:26:21.682 align:middle line:-2
以上就是图像分类的内容
接下来 我们将讨论对象识别

00:26:24.518 --> 00:26:27.120 align:middle line:-2
一点警告
在这个演示中 一个活羊角面包

00:26:27.187 --> 00:26:28.455 align:middle line:-1
可能会在舞台上受伤

00:26:28.522 --> 00:26:30.424 align:middle line:-2
所以对于那些娇气的人
请把目光移开

00:26:34.528 --> 00:26:38.699 align:middle line:-1
我们用于对象识别的模型

00:26:38.765 --> 00:26:42.703 align:middle line:-2
基于这种YOLO技术
即“你只看一次”

00:26:43.036 --> 00:26:45.072 align:middle line:-1
这是一个运行非常快速的模型

00:26:45.138 --> 00:26:50.310 align:middle line:-2
它允许我们获取对象的边界框
并为其添加标签

00:26:50.844 --> 00:26:52.513 align:middle line:-1
并且它可以在屏幕上找到多个对象

00:26:52.579 --> 00:26:54.014 align:middle line:-1
如这幅截图所示

00:26:57.451 --> 00:27:01.054 align:middle line:-2
它的优势在于我可以得到
比如它们的位置

