WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:26:58.756 --> 00:27:01.396 A:middle
map, you will need to scan your

00:27:01.466 --> 00:27:03.966 A:middle
full physical space, something

00:27:03.966 --> 00:27:06.286 A:middle
like a 360-degree scan you do

00:27:06.286 --> 00:27:07.376 A:middle
with your panoramas.

00:27:08.586 --> 00:27:10.856 A:middle
But this is not practical for

00:27:10.956 --> 00:27:11.516 A:middle
end-users.

00:27:13.296 --> 00:27:15.526 A:middle
So ARKit makes it very easy for

00:27:15.526 --> 00:27:18.626 A:middle
you by automatically completing

00:27:18.626 --> 00:27:20.756 A:middle
this cube map using advanced

00:27:20.756 --> 00:27:21.946 A:middle
machine learning algorithms.

00:27:24.016 --> 00:27:29.460 A:middle
[ Applause ]

00:27:31.536 --> 00:27:33.446 A:middle
Note also that all of this

00:27:33.446 --> 00:27:35.296 A:middle
processing happens locally on

00:27:35.296 --> 00:27:37.416 A:middle
your device in real-time.

00:27:39.556 --> 00:27:41.666 A:middle
So once we have a cube map, we

00:27:41.666 --> 00:27:44.116 A:middle
can set up reflection probe and

00:27:44.116 --> 00:27:46.066 A:middle
as soon as we place virtual

00:27:46.066 --> 00:27:48.186 A:middle
objects in the scene, they start

00:27:48.186 --> 00:27:49.506 A:middle
to reflect the real environment.

00:27:50.006 --> 00:27:52.726 A:middle
So this was a quick overview of

00:27:52.726 --> 00:27:54.416 A:middle
how this environment texturing

00:27:54.416 --> 00:27:55.116 A:middle
process works.

00:27:55.116 --> 00:28:00.286 A:middle
Let's see how ARKit API makes it

