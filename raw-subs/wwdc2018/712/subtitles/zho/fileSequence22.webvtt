WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:22:00.046 --> 00:22:01.686 A:middle
所以在访达里

00:22:01.686 --> 00:22:03.316 A:middle
我已经得到了 BreakfastModel.mlmodel

00:22:03.316 --> 00:22:05.636 A:middle
当我在 Xcode 中打开它时

00:22:05.636 --> 00:22:07.106 A:middle
它看起来就像

00:22:07.106 --> 00:22:08.306 A:middle
任何 Core ML 模型

00:22:08.896 --> 00:22:12.176 A:middle
它需要一个输入图像

00:22:12.176 --> 00:22:15.206 A:middle
然后输出置信度和坐标

00:22:15.616 --> 00:22:16.706 A:middle
这将告诉我们

00:22:16.706 --> 00:22:18.336 A:middle
预测的边界框和

00:22:18.336 --> 00:22:19.436 A:middle
图像的标签

00:22:20.376 --> 00:22:22.206 A:middle
现在让我们切换到

00:22:22.246 --> 00:22:23.646 A:middle
iPhone App

00:22:23.646 --> 00:22:24.616 A:middle
我们要使用这个模型

00:22:25.186 --> 00:22:30.306 A:middle
在我的 iPhone 上

00:22:30.306 --> 00:22:31.716 A:middle
有一个名为 Food Predictor 的 App

00:22:32.356 --> 00:22:33.336 A:middle
它使用的是

00:22:33.336 --> 00:22:34.456 A:middle
我们刚刚训练的模型

00:22:35.356 --> 00:22:37.096 A:middle
在这里 我要选择 相片

00:22:37.416 --> 00:22:39.026 A:middle
我有今天早上

00:22:39.026 --> 00:22:40.036 A:middle
早餐的照片

00:22:40.546 --> 00:22:41.506 A:middle
这是我非常典型的早餐

00:22:41.506 --> 00:22:43.326 A:middle
包含咖啡和香蕉

00:22:43.796 --> 00:22:45.546 A:middle
好吧 我经常不吃香蕉

00:22:46.316 --> 00:22:48.856 A:middle
假设今天早上我吃了

00:22:48.856 --> 00:22:49.346 A:middle
一根香蕉吧

00:22:50.456 --> 00:22:53.796 A:middle
我们可以直接点击图像

00:22:53.796 --> 00:22:54.976 A:middle
因为我们看到了边界框

00:22:54.976 --> 00:22:58.386 A:middle
我们可以识别边界框内的对象

00:22:58.386 --> 00:22:59.716 A:middle
我们看到模型告诉我们

00:22:59.716 --> 00:23:02.976 A:middle
这是一根香蕉 这是一杯咖啡

