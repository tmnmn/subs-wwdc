WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

00:05:59.686 --> 00:06:01.916 A:middle
making a custom image classifier

00:06:02.546 --> 00:06:04.066 A:middle
that is going to be used by

00:06:04.066 --> 00:06:06.486 A:middle
users on their iPhone, so

00:06:06.486 --> 00:06:07.946 A:middle
collect pictures from your

00:06:07.946 --> 00:06:08.236 A:middle
iPhone.

00:06:08.676 --> 00:06:10.246 A:middle
Do not collect -- collect less

00:06:10.366 --> 00:06:12.566 A:middle
screenshots but have more iPhone

00:06:13.106 --> 00:06:13.326 A:middle
pictures.

00:06:14.536 --> 00:06:17.606 A:middle
Then you train your model.

00:06:18.396 --> 00:06:21.276 A:middle
Finally, an important step here

00:06:21.276 --> 00:06:22.756 A:middle
is to evaluate this model.

00:06:23.366 --> 00:06:25.686 A:middle
The model evaluation is done on

00:06:25.686 --> 00:06:27.996 A:middle
a separate handout set.

00:06:29.286 --> 00:06:31.556 A:middle
If you're happy, you write out

00:06:31.606 --> 00:06:32.376 A:middle
the ML model.

00:06:33.726 --> 00:06:35.416 A:middle
But let's just say the results

00:06:35.416 --> 00:06:36.216 A:middle
are not good.

00:06:36.216 --> 00:06:37.246 A:middle
You should either retrain your

00:06:37.246 --> 00:06:39.416 A:middle
model with different parameters

00:06:39.686 --> 00:06:41.676 A:middle
or you collect more data.

00:06:43.456 --> 00:06:45.356 A:middle
Create ML actually helps you

00:06:45.356 --> 00:06:47.736 A:middle
across all four stages of this

00:06:47.736 --> 00:06:48.176 A:middle
workflow.

00:06:48.796 --> 00:06:52.886 A:middle
We have powerful in-built data

00:06:52.886 --> 00:06:53.846 A:middle
[inaudible] utilities, data

00:06:53.946 --> 00:06:55.816 A:middle
source and data table that we

00:06:55.816 --> 00:06:57.196 A:middle
will talk in the remainder of

00:06:57.196 --> 00:06:58.036 A:middle
the presentation.

00:06:58.466 --> 00:07:01.546 A:middle
You can actually train your

