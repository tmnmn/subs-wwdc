WEBVTT
X-TIMESTAMP-MAP=MPEGTS:181083,LOCAL:00:00:00.000

01:27:56.576 --> 01:28:00.706 A:middle
ARKit 2

01:28:00.706 --> 01:28:03.076 A:middle
ARKit 2 提供了很多的改进

01:28:03.076 --> 01:28:06.116 A:middle
包括提升的

01:28:06.116 --> 01:28:07.946 A:middle
面部跟踪功能

01:28:07.946 --> 01:28:11.826 A:middle
新增了注视和舌头的跟踪能力

01:28:12.386 --> 01:28:13.786 A:middle
这些高要求的功能

01:28:13.786 --> 01:28:14.766 A:middle
允许你们把面部动画的逼真程度

01:28:14.766 --> 01:28:16.726 A:middle
带到一个新的水平

01:28:17.566 --> 01:28:18.846 A:middle
事实证明

01:28:18.846 --> 01:28:20.706 A:middle
孩子们在玩动画表情时的

01:28:20.706 --> 01:28:22.226 A:middle
第一件事就是伸出舌头

01:28:22.226 --> 01:28:23.536 A:middle
我想你们很多人也是这样

01:28:23.536 --> 01:28:24.886 A:middle
这就是为什么 我们要新增这项功能

01:28:28.366 --> 01:28:29.956 A:middle
为了更准确地将物体

01:28:29.956 --> 01:28:31.636 A:middle
融入到场景中

01:28:31.636 --> 01:28:33.346 A:middle
我们添加了环境纹理贴图

01:28:33.726 --> 01:28:36.206 A:middle
ARKit 基于摄像头在现实世界中

01:28:36.206 --> 01:28:37.986 A:middle
看到的场景创建纹理贴图

01:28:37.986 --> 01:28:41.346 A:middle
请注意 这个球反射的是

01:28:41.346 --> 01:28:42.466 A:middle
桌子上的那张真实的图片

01:28:43.046 --> 01:28:44.966 A:middle
但是摄像头看不到的场景

01:28:44.966 --> 01:28:45.706 A:middle
该怎么办呢

01:28:46.346 --> 01:28:47.896 A:middle
通过机器学习

01:28:47.896 --> 01:28:49.286 A:middle
我们在数千个典型的环境中

01:28:49.456 --> 01:28:51.356 A:middle
训练了一个神经网络

01:28:51.786 --> 01:28:53.826 A:middle
这使得 ARKit 能够

01:28:53.826 --> 01:28:55.876 A:middle
虚构场景的其他部分

01:28:56.406 --> 01:28:57.336 A:middle
这意味着你们会得到一些

01:28:57.416 --> 01:28:58.906 A:middle
十分可信的反射

01:28:58.906 --> 01:28:59.846 A:middle
比如头顶的灯光

01:28:59.846 --> 01:29:01.026 A:middle
即使摄像头根本没有

